{
 "metadata": {
  "name": "",
  "signature": "sha256:f92cda6a42905321f38d61e1d263cad7a90f49a056e0ed93a73e77b2d06da37c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Exploration of Rank-Deficient Least Squares\n",
      "* Matthew Cohen - April 23, 2015"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the process of exploring Bark domain filter design, the main issue that I was confronted with was a rank-deficient matrix in the least squares problem.\n",
      "\n",
      "When considering the frequency-domain least squares approach, when applying the Bark transform to the problem formulation $\\textbf{BCa} \\approx \\textbf{BD}$, the analytic solution can be formulated as:\n",
      "\n",
      "$$\\textbf{a} = ((BC)^{T}BC)^{-1} (BC)^{T} \\textbf{BD}$$\n",
      "\n",
      "This assumes that matrix $BC$ is \"invertible,\" i.e. the Moore-Penrose pseudoinverse of BC exists and can be computed. However, the rank deficiency of $BC$ makes this not work in the end.\n",
      "\n",
      "Before exploring more into the problem involving the Bark domain filter design problem, I will first explore matrix inversion in some detail."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Matrix Inversion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\textbf{A}$ be a square **n x n** matrix. If $\\textbf{A}$ is invertible, then $\\textbf{A}$ has full rank, i.e. rank($\\textbf{A}$) = n (the columns of $\\textbf{A}$ are linearly independent).\n",
      "\n",
      "Now, consider a non-square matrix $\\textbf{A}$ that is **m x n**. Non-square matrices don't have inverses. However, the Moore-Penrose pseudo-inverse can be defined for an **m x n** matrix (m and n not equal). This pseudo-inverse of $\\textbf{A}$ is defined as an **n x m** matrix $A^{+}$.\n",
      "\n",
      "Matrix $A^{+}$ exists for any matrix A, but when the latter has full rank, $A^{+}$ can be expressed as a simple algebraic formula.\n",
      "* When A has full column rank (matrix $A^{T}A$ is invertible), then $A^{+} = (A^{T}A)^{-1}A^{T}$ (left inverse)\n",
      "* When A has full row rank (matrix $AA^{T}$ is invertible), then $A^{+} = A^{T}(AA^{T})^{-1}$ (right inverse)\n",
      "\n",
      "However, when A is not full rank, then these formulas cannot be used. More generally, the pseudo-inverse is best computed using the Singular Value Decomposition (SVD)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Singular Value Decomposition (SVD)\n",
      "\n",
      "Let $\\textbf{A}$ be an **m x n** real matrix. Then there exists orthogonal matrices $\\textbf{U}$ (**m x m**) and $\\textbf{V}$ (**n x n**) such that matrix $\\textbf{A}$ can be decomposed as follows, using the SVD:\n",
      "\n",
      "$$\\textbf{A} = \\textbf{U} \\Sigma V^{T}$$\n",
      "\n",
      "where $\\Sigma$ is an **m x n** diagonal matrix having form\n",
      "\n",
      "$$\\Sigma = \n",
      "\\begin{bmatrix}\n",
      "    \\sigma_{1}      & 0 & 0 & \\dots & 0 & 0 \\\\\n",
      "    0      & \\sigma_{2} & 0 & \\dots & 0 & 0 \\\\\n",
      "    0      & 0 & \\sigma_{3} & \\dots & 0 & 0 \\\\\n",
      "    ...       & ... & ... & ... \\\\\n",
      "    0      & 0 & 0 & \\dots & \\sigma_{p} & 0 \\\\\n",
      "\\end{bmatrix}$$ \n",
      "\n",
      "where $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{p} \\geq 0$, $p = \\min(m,n)$, and ${\\sigma_{i}}$ are the singular values of A. The columns of $U$ are the left singular vectors, and the columns of $V$ are the right singular vectors. If matrix A is rank deficient, then one or more of the singular values will be zero.\n",
      "\n",
      "Using the SVD, the pseudo-inverse of a matrix can be easily computed. Let A be decomposed as $\\textbf{A} = \\textbf{U} \\Sigma V^{T}$. Then:\n",
      "\n",
      "$$A^{+} = \\textbf{V} \\Sigma^{+} U^{T}$$\n",
      "\n",
      "where \\Sigma^{+} takes the form\n",
      "\n",
      "$$\\Sigma^{+} = \n",
      "\\begin{bmatrix}\n",
      "    1/\\sigma_{1}      & 0 & 0 & \\dots & 0 & 0 \\\\\n",
      "    0      & 1/\\sigma_{2} & 0 & \\dots & 0 & 0 \\\\\n",
      "    0      & 0 & 1/\\sigma_{3} & \\dots & 0 & 0 \\\\\n",
      "    ...       & ... & ... & ... \\\\\n",
      "    0      & 0 & 0 & \\dots & 1/\\sigma_{p} & 0 \\\\\n",
      "\\end{bmatrix}$$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Rank Deficient Least Squares using SVD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\textbf{A}$ be an **m x n** real matrix with SVD $\\textbf{A} = \\textbf{U} \\Sigma V^{T}$. Let $\\textbf{b}$ be an **m x 1** vector. Assume k = rank($\\textbf{A}$).\n",
      "\n",
      "Assuming $\\Sigma_{1}$ is the leading submatrix of $\\Sigma$ corresponding to k positive singular values, the compact SVD of $\\textbf{A}$ can be written as:\n",
      "\n",
      "$$\\textbf{A} = U_{1} \\Sigma V_{1}^{T}$$\n",
      "\n",
      "where $U_{1} = [u_{1}, u_{2}, \\dots , u_{k}]$, $V_{1} = [v_{1}, v_{2}, \\dots , v_{k}]$, and\n",
      "\n",
      "$$\\Sigma_{1} = \n",
      "\\begin{bmatrix}\n",
      "    \\sigma_{1}      & 0 & 0 & \\dots & 0 \\\\\n",
      "    0      & \\sigma_{2} & 0 & \\dots & 0 \\\\\n",
      "    0      & 0 & \\sigma_{3} & \\dots & 0 \\\\\n",
      "    ...       & ... & ... & ... \\\\\n",
      "    0      & 0 & 0 & \\dots & \\sigma_{k} \\\\\n",
      "\\end{bmatrix}$$ \n",
      "\n",
      "where $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{k} \\geq 0$.\n",
      "\n",
      "The *minimum-norm solution* to the linear least squares problem is given by:\n",
      "\n",
      "$$\\hat{x} = A^{+} \\textbf{b} = V_{1} \\Sigma_{1}^{-1} U_{1}^{T} \\textbf{b}$$\n",
      "\n",
      "where $A^{+} = V_{1} \\Sigma_{1}^{-1} U_{1}^{T}$ is the pseudo-inverse of $\\textbf{A}$.\n",
      "\n",
      "So, given a situation where you are attempting to solve a linear least squares problem with a rank deficient, non-invertible matrix, the system can be solved by determining the SVD of the matrix, and then using this SVD to compute the pseudo-inverse needed for the least squares solution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}